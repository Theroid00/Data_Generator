"""
Unified AI Generator
The heart of the synthetic data system - ONE powerful AI model with template interfaces
"""

import pandas as pd
import numpy as np
import pickle
import logging
import json
from pathlib import Path
from typing import Optional, Union, Dict, List, Any
from datetime import datetime
import warnings

try:
    from sdv.single_table import CTGANSynthesizer, GaussianCopulaSynthesizer
    SDV_AVAILABLE = True
except ImportError:
    SDV_AVAILABLE = False

from logic_rules import DataValidator, RelationshipEnforcer


class UnifiedAIGenerator:
    """
    The UNIFIED AI GENERATOR - Train once, use everywhere!
    
    This is the core intelligence that learned from ALL datasets.
    Templates are just convenient ways to interact with this power.
    """
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.validator = DataValidator()
        self.relationship_enforcer = RelationshipEnforcer()
        self.unified_model = None
        self.unified_model_path = Path('output') / 'trained_model_UNIFIED_ALL_DATASETS.pkl'
        
        # Load the unified model if available
        self.load_unified_model()
    
    def load_unified_model(self) -> bool:
        """Load the pre-trained unified AI model"""
        try:
            if not self.unified_model_path.exists():
                self.logger.warning(f"Unified model not found at {self.unified_model_path}")
                return False
            
            with open(self.unified_model_path, 'rb') as f:
                self.unified_model = pickle.load(f)
            
            self.logger.info("ðŸ§  UNIFIED AI MODEL LOADED - Ready to generate ANY data type!")
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to load unified model: {str(e)}")
            return False
    
    def is_ready(self) -> bool:
        """Check if the unified AI model is ready to use"""
        return self.unified_model is not None
    
    def generate_from_template(self, template_path: Union[str, Path], num_rows: int) -> Optional[pd.DataFrame]:
        """
        Generate data using template as interface to unified AI model
        
        The template defines WHAT columns to generate, the AI model provides the INTELLIGENCE
        """
        try:
            if not self.is_ready():
                self.logger.error("Unified AI model not loaded! Run training first.")
                return None
            
            # Load template
            with open(template_path, 'r') as f:
                template = json.load(f)
            
            self.logger.info(f"ðŸŽ¯ Generating {num_rows} rows using template: {template['name']}")
            self.logger.info("ðŸ§  Powered by UNIFIED AI MODEL (cross-domain intelligence)")
            
            # Generate base data from AI model
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                
                # Generate more rows than needed, then sample to get exact count
                # This gives better variety from the AI model
                oversample_factor = min(2.0, max(1.2, 500 / num_rows))
                ai_rows = int(num_rows * oversample_factor)
                
                raw_ai_data = self.unified_model.sample(ai_rows)
            
            # Transform AI output to match template specification
            template_data = self._transform_ai_data_to_template(raw_ai_data, template, num_rows)
            
            if template_data is None:
                return None
            
            # Apply template-specific relationships and logic
            template_data = self._apply_template_logic(template_data, template)
            
            # Final validation and cleanup
            template_data = self.relationship_enforcer.enforce_relationships(template_data)
            
            self.logger.info(f"âœ… Generated {len(template_data)} rows with template '{template['name']}'")
            return template_data
            
        except Exception as e:
            self.logger.error(f"Failed to generate from template: {str(e)}")
            return None
    
    def _transform_ai_data_to_template(self, ai_data: pd.DataFrame, template: Dict, target_rows: int) -> Optional[pd.DataFrame]:
        """Transform AI model output to match template column specifications"""
        try:
            # Sample the exact number of rows we need
            if len(ai_data) > target_rows:
                ai_data = ai_data.sample(n=target_rows, random_state=42).reset_index(drop=True)
            elif len(ai_data) < target_rows:
                # Repeat data to reach target count
                repeats = (target_rows // len(ai_data)) + 1
                ai_data = pd.concat([ai_data] * repeats, ignore_index=True)
                ai_data = ai_data.iloc[:target_rows].reset_index(drop=True)
            
            template_data = pd.DataFrame()
            template_columns = template['columns']
            ai_columns = list(ai_data.columns)
            
            for col_spec in template_columns:
                col_name = col_spec['name']
                col_type = col_spec['data_type']
                
                # Try to find similar column in AI data
                source_col = self._find_best_ai_column(col_name, col_type, ai_columns, ai_data)
                
                if source_col and source_col in ai_data.columns:
                    # Use AI data as base, then transform to match template spec
                    template_data[col_name] = self._transform_column(
                        ai_data[source_col], col_spec
                    )
                else:
                    # Generate new column based on template spec (fallback)
                    template_data[col_name] = self._generate_template_column(col_spec, target_rows)
            
            return template_data
            
        except Exception as e:
            self.logger.error(f"Failed to transform AI data: {str(e)}")
            return None
    
    def _find_best_ai_column(self, target_name: str, target_type: str, ai_columns: List[str], ai_data: pd.DataFrame) -> Optional[str]:
        """Find the best matching column from AI data"""
        # Direct name match
        if target_name in ai_columns:
            return target_name
        
        # Semantic matching for common patterns
        name_lower = target_name.lower()
        type_mappings = {
            'name': ['name', 'first_name', 'last_name', 'full_name'],
            'email': ['email', 'email_address'],
            'integer': ['id', 'age', 'salary', 'count', 'year', 'number'],
            'categorical': ['department', 'category', 'type', 'status', 'level'],
            'float': ['rate', 'score', 'rating', 'percentage', 'ratio']
        }
        
        # Look for semantic matches
        if target_type in type_mappings:
            for pattern in type_mappings[target_type]:
                if pattern in name_lower:
                    for ai_col in ai_columns:
                        if pattern in ai_col.lower():
                            return ai_col
        
        # Look for type-compatible columns
        for ai_col in ai_columns:
            ai_col_type = str(ai_data[ai_col].dtype)
            if target_type == 'integer' and ('int' in ai_col_type):
                return ai_col
            elif target_type == 'float' and ('float' in ai_col_type):
                return ai_col
            elif target_type == 'categorical' and ('object' in ai_col_type or 'string' in ai_col_type):
                return ai_col
        
        # Return first numeric column for numeric types, first text for others
        if target_type in ['integer', 'float']:
            numeric_cols = ai_data.select_dtypes(include=[np.number]).columns
            return numeric_cols[0] if len(numeric_cols) > 0 else None
        else:
            text_cols = ai_data.select_dtypes(include=['object', 'string']).columns  
            return text_cols[0] if len(text_cols) > 0 else None
    
    def _transform_column(self, ai_column: pd.Series, col_spec: Dict) -> pd.Series:
        """Transform AI column to match template specification"""
        try:
            col_type = col_spec['data_type']
            
            if col_type == 'integer':
                # Convert to integer within specified range
                numeric_data = pd.to_numeric(ai_column, errors='coerce')
                # Handle case where all values are the same (avoid division by zero)
                if numeric_data.max() == numeric_data.min():
                    if 'min_value' in col_spec and 'max_value' in col_spec:
                        return pd.Series([np.random.randint(col_spec['min_value'], col_spec['max_value'] + 1) 
                                        for _ in range(len(ai_column))])
                    else:
                        return numeric_data.fillna(0).round().astype(int)
                
                if 'min_value' in col_spec and 'max_value' in col_spec:
                    # Scale to target range
                    min_val, max_val = col_spec['min_value'], col_spec['max_value']
                    range_original = numeric_data.max() - numeric_data.min()
                    scaled = ((numeric_data - numeric_data.min()) / range_original * 
                             (max_val - min_val) + min_val)
                    return scaled.fillna(min_val).round().astype(int)
                else:
                    return numeric_data.fillna(0).round().astype(int)
            
            elif col_type == 'float':
                # Convert to float within specified range
                numeric_data = pd.to_numeric(ai_column, errors='coerce')
                # Handle case where all values are the same (avoid division by zero)
                if numeric_data.max() == numeric_data.min():
                    if 'min_value' in col_spec and 'max_value' in col_spec:
                        return pd.Series([np.random.uniform(col_spec['min_value'], col_spec['max_value']) 
                                        for _ in range(len(ai_column))])
                    else:
                        return numeric_data.fillna(0.0).round(2)
                
                if 'min_value' in col_spec and 'max_value' in col_spec:
                    min_val, max_val = col_spec['min_value'], col_spec['max_value']
                    range_original = numeric_data.max() - numeric_data.min()
                    scaled = ((numeric_data - numeric_data.min()) / range_original * 
                             (max_val - min_val) + min_val)
                    return scaled.fillna(min_val).round(2)  # Round to 2 decimal places
                else:
                    return numeric_data.fillna(0.0).round(2)
            
            elif col_type == 'categorical':
                # Map to specified categories
                if 'categories' in col_spec:
                    categories = col_spec['categories']
                    # Use AI data as inspiration but constrain to template categories
                    return pd.Series([np.random.choice(categories) for _ in range(len(ai_column))])
                else:
                    return ai_column.astype(str)
            
            elif col_type == 'name':
                # Generate names (AI model gives us realistic patterns)
                from faker import Faker
                fake = Faker()
                col_name = col_spec.get('name', '').lower()
                if 'first_name' in col_name:
                    return pd.Series([fake.first_name() for _ in range(len(ai_column))])
                elif 'last_name' in col_name:
                    return pd.Series([fake.last_name() for _ in range(len(ai_column))])
                else:
                    return pd.Series([fake.name() for _ in range(len(ai_column))])
            
            elif col_type == 'email':
                # Generate emails
                from faker import Faker
                fake = Faker()
                return pd.Series([fake.email() for _ in range(len(ai_column))])
            
            elif col_type == 'uuid':
                # Generate realistic UUIDs
                import uuid
                import random
                import string
                from datetime import datetime
                
                uuid_format = col_spec.get('uuid_format', 'short')
                uuid_prefix = col_spec.get('uuid_prefix', '')
                uuid_length = col_spec.get('uuid_length', 8)
                
                if uuid_format == 'short':
                    # Short alphanumeric IDs: A7D3E41B
                    return pd.Series([''.join(random.choices(string.ascii_uppercase + string.digits, k=8)) 
                                     for _ in range(len(ai_column))])
                
                elif uuid_format == 'medium':
                    # Medium format with dashes: A7D3-E41B-9F2C
                    return pd.Series([f"{''.join(random.choices(string.ascii_uppercase + string.digits, k=4))}-"
                                     f"{''.join(random.choices(string.ascii_uppercase + string.digits, k=4))}-"
                                     f"{''.join(random.choices(string.ascii_uppercase + string.digits, k=4))}"
                                     for _ in range(len(ai_column))])
                
                elif uuid_format == 'readable':
                    # Readable format: USER-2024-001234
                    year = datetime.now().year
                    prefix = uuid_prefix or 'ID'
                    return pd.Series([f"{prefix}-{year}-{str(i+1).zfill(6)}" for i in range(len(ai_column))])
                
                elif uuid_format == 'sequential':
                    # Sequential format: ID000001, ID000002
                    prefix = uuid_prefix or 'ID'
                    return pd.Series([f"{prefix}{str(i+1).zfill(6)}" for i in range(len(ai_column))])
                
                elif uuid_format == 'custom':
                    # Custom length with prefix
                    prefix = uuid_prefix or ''
                    length = max(4, min(16, uuid_length))
                    return pd.Series([f"{prefix}{''.join(random.choices(string.ascii_uppercase + string.digits, k=length))}"
                                     for _ in range(len(ai_column))])
                
                else:  # fallback to short
                    return pd.Series([''.join(random.choices(string.ascii_uppercase + string.digits, k=8)) 
                                     for _ in range(len(ai_column))])
            
            elif col_type == 'text':
                # For date fields, clean up the format
                col_name = col_spec.get('name', '').lower()
                if 'date' in col_name:
                    from faker import Faker
                    fake = Faker()
                    return pd.Series([fake.date_between(start_date='-10y', end_date='today').strftime('%Y-%m-%d') for _ in range(len(ai_column))])
                else:
                    return ai_column.astype(str)
            
            else:
                # Default: return as string
                return ai_column.astype(str)
                
        except Exception as e:
            self.logger.warning(f"Column transformation failed: {e}")
            return ai_column
    
    def _generate_template_column(self, col_spec: Dict, num_rows: int) -> pd.Series:
        """Generate column data based on template specification (fallback method)"""
        try:
            col_type = col_spec['data_type']
            col_name = col_spec['name']
            
            if col_type == 'integer':
                min_val = col_spec.get('min_value', 1)
                max_val = col_spec.get('max_value', 1000)
                return pd.Series(np.random.randint(min_val, max_val + 1, num_rows))
            
            elif col_type == 'float':
                min_val = col_spec.get('min_value', 0.0)
                max_val = col_spec.get('max_value', 1.0)
                return pd.Series(np.random.uniform(min_val, max_val, num_rows))
            
            elif col_type == 'categorical':
                categories = col_spec.get('categories', ['Category1', 'Category2', 'Category3'])
                return pd.Series(np.random.choice(categories, num_rows))
            
            elif col_type == 'name':
                from faker import Faker
                fake = Faker()
                if 'first_name' in col_name.lower():
                    return pd.Series([fake.first_name() for _ in range(num_rows)])
                elif 'last_name' in col_name.lower():
                    return pd.Series([fake.last_name() for _ in range(num_rows)])
                else:
                    return pd.Series([fake.name() for _ in range(num_rows)])
            
            elif col_type == 'email':
                from faker import Faker
                fake = Faker()
                return pd.Series([fake.email() for _ in range(num_rows)])
            
            elif col_type == 'uuid':
                import uuid
                import random
                import string
                from datetime import datetime
                
                uuid_format = col_spec.get('uuid_format', 'short')
                uuid_prefix = col_spec.get('uuid_prefix', '')
                uuid_length = col_spec.get('uuid_length', 8)
                
                if uuid_format == 'short':
                    # Short alphanumeric IDs: A7D3E41B
                    return pd.Series([''.join(random.choices(string.ascii_uppercase + string.digits, k=8)) 
                                     for _ in range(num_rows)])
                
                elif uuid_format == 'medium':
                    # Medium format with dashes: A7D3-E41B-9F2C
                    return pd.Series([f"{''.join(random.choices(string.ascii_uppercase + string.digits, k=4))}-"
                                     f"{''.join(random.choices(string.ascii_uppercase + string.digits, k=4))}-"
                                     f"{''.join(random.choices(string.ascii_uppercase + string.digits, k=4))}"
                                     for _ in range(num_rows)])
                
                elif uuid_format == 'readable':
                    # Readable format: USER-2024-001234
                    year = datetime.now().year
                    prefix = uuid_prefix or 'ID'
                    return pd.Series([f"{prefix}-{year}-{str(i+1).zfill(6)}" for i in range(num_rows)])
                
                elif uuid_format == 'sequential':
                    # Sequential format: ID000001, ID000002
                    prefix = uuid_prefix or 'ID'
                    return pd.Series([f"{prefix}{str(i+1).zfill(6)}" for i in range(num_rows)])
                
                elif uuid_format == 'custom':
                    # Custom length with prefix
                    prefix = uuid_prefix or ''
                    length = max(4, min(16, uuid_length))
                    return pd.Series([f"{prefix}{''.join(random.choices(string.ascii_uppercase + string.digits, k=length))}"
                                     for _ in range(num_rows)])
                
                else:  # fallback to short
                    return pd.Series([''.join(random.choices(string.ascii_uppercase + string.digits, k=8)) 
                                     for _ in range(num_rows)])
            
            else:
                # Default text
                return pd.Series([f"{col_name}_{i}" for i in range(num_rows)])
                
        except Exception as e:
            self.logger.error(f"Failed to generate column {col_spec['name']}: {e}")
            return pd.Series([f"Error_{i}" for i in range(num_rows)])
    
    def _apply_template_logic(self, data: pd.DataFrame, template: Dict) -> pd.DataFrame:
        """Apply template-specific business logic and relationships"""
        try:
            # Apply any relationships defined in template
            if 'relationships' in template and template['relationships']:
                for relationship in template['relationships']:
                    # Apply custom relationship logic here
                    pass
            
            # Fix name-email consistency FIRST
            if 'first_name' in data.columns and 'last_name' in data.columns and 'email' in data.columns:
                for idx in data.index:
                    first_name = str(data.loc[idx, 'first_name']).lower().replace(' ', '')
                    last_name = str(data.loc[idx, 'last_name']).lower().replace(' ', '')
                    # Create realistic email from actual names
                    email_formats = [
                        f"{first_name}.{last_name}@company.com",
                        f"{first_name}{last_name}@company.com", 
                        f"{first_name[0]}{last_name}@company.com",
                        f"{first_name}.{last_name}@example.com"
                    ]
                    data.loc[idx, 'email'] = np.random.choice(email_formats)
            
            # Apply template-specific data quality improvements
            template_name = template.get('name', '').lower()
            
            if 'employee' in template_name:
                data = self._apply_employee_logic(data)
            elif 'customer' in template_name:
                data = self._apply_customer_logic(data)
            elif 'product' in template_name:
                data = self._apply_product_logic(data)
            
            # Final cleanup: ensure integer columns are properly formatted
            for col in data.columns:
                if data[col].dtype in ['float64', 'float32']:
                    # Check if this should be an integer (salary, age, etc.)
                    if col.lower() in ['salary', 'age', 'employee_id', 'years_experience', 'price', 'cost']:
                        try:
                            data[col] = data[col].round().astype(int)
                        except:
                            pass
            
            return data
            
        except Exception as e:
            self.logger.warning(f"Failed to apply template logic: {e}")
            return data
    
    def _apply_employee_logic(self, data: pd.DataFrame) -> pd.DataFrame:
        """Apply employee-specific business logic"""
        try:
            # Salary vs experience correlation
            if 'salary' in data.columns and 'years_experience' in data.columns:
                base_salary = 50000
                data['salary'] = base_salary + (data['years_experience'] * 2000) + np.random.randint(-5000, 10000, len(data))
                data['salary'] = data['salary'].clip(lower=30000)
            
            # Age vs experience consistency
            if 'age' in data.columns and 'years_experience' in data.columns:
                data['years_experience'] = np.minimum(data['years_experience'], data['age'] - 18)
                data['years_experience'] = data['years_experience'].clip(lower=0)
            
            return data
        except Exception as e:
            self.logger.warning(f"Employee logic failed: {e}")
            return data
    
    def _apply_customer_logic(self, data: pd.DataFrame) -> pd.DataFrame:
        """Apply customer-specific business logic"""
        try:
            # Customer lifetime value correlations
            if 'total_spent' in data.columns and 'years_as_customer' in data.columns:
                data['total_spent'] = data['years_as_customer'] * np.random.uniform(100, 1000, len(data))
            
            return data
        except Exception as e:
            self.logger.warning(f"Customer logic failed: {e}")
            return data
    
    def _apply_product_logic(self, data: pd.DataFrame) -> pd.DataFrame:
        """Apply product-specific business logic"""
        try:
            # Price vs rating correlation
            if 'price' in data.columns and 'rating' in data.columns:
                # Higher priced items tend to have slightly higher ratings
                price_factor = (data['price'] - data['price'].min()) / (data['price'].max() - data['price'].min())
                data['rating'] = 3.0 + (price_factor * 2.0) + np.random.normal(0, 0.5, len(data))
                data['rating'] = data['rating'].clip(1.0, 5.0).round(1)

            return data
        except Exception as e:
            self.logger.warning(f"Product logic failed: {e}")
            return data
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get information about the loaded model"""
        try:
            if not self.is_ready():
                return {
                    'status': 'Not Ready',
                    'message': 'Unified AI model not loaded',
                    'model_type': 'None',
                    'model_size_mb': 0,
                    'training_data': 'N/A'
                }

            # Get model file size
            model_size_bytes = self.unified_model_path.stat().st_size
            model_size_mb = round(model_size_bytes / (1024 * 1024), 1)

            # Determine model type
            model_type = type(self.unified_model).__name__ if self.unified_model else 'Unknown'

            return {
                'status': 'Ready',
                'message': 'UNIFIED AI MODEL - Trained on cross-domain datasets',
                'model_type': model_type,
                'model_size_mb': model_size_mb,
                'training_data': 'Multiple industry datasets (employees, customers, products, etc.)'
            }

        except Exception as e:
            self.logger.error(f"Failed to get model info: {e}")
            return {
                'status': 'Error',
                'message': f'Error getting model info: {e}',
                'model_type': 'Unknown',
                'model_size_mb': 0,
                'training_data': 'Unknown'
            }

    def list_available_templates(self) -> List[Path]:
        """List all available template files"""
        try:
            templates_dir = Path('templates')
            if not templates_dir.exists():
                templates_dir.mkdir(exist_ok=True)
                return []

            # Find all JSON template files
            template_files = list(templates_dir.glob('*.json'))

            # Sort by name for consistent ordering
            template_files.sort(key=lambda x: x.name)

            return template_files

        except Exception as e:
            self.logger.error(f"Failed to list templates: {e}")
            return []

    def generate_pure_ai(self, num_rows: int, columns: Optional[List[str]] = None) -> Optional[pd.DataFrame]:
        """
        Generate pure AI data without template constraints
        This unleashes the full power of the unified model
        """
        try:
            if not self.is_ready():
                self.logger.error("Unified AI model not ready for pure generation")
                return None
            
            self.logger.info(f"ðŸ§  PURE AI MODE - Generating {num_rows} rows of unrestricted cross-domain data")

            with warnings.catch_warnings():
                warnings.simplefilter("ignore")

                # Generate raw AI data
                raw_data = self.unified_model.sample(num_rows)

            # If specific columns requested, filter to those
            if columns:
                available_cols = [col for col in columns if col in raw_data.columns]
                if available_cols:
                    raw_data = raw_data[available_cols]

            # Apply basic relationships
            raw_data = self.relationship_enforcer.enforce_relationships(raw_data)

            self.logger.info(f"âœ… Pure AI generation complete: {len(raw_data)} rows, {len(raw_data.columns)} columns")
            return raw_data

        except Exception as e:
            self.logger.error(f"Pure AI generation failed: {e}")
            return None
    
    def analyze_template_quality(self, template_path: Union[str, Path]) -> Dict[str, Any]:
        """Analyze the quality and completeness of a template"""
        try:
            with open(template_path, 'r') as f:
                template = json.load(f)

            quality_score = 0
            max_score = 100

            # Check required fields (30 points)
            required_fields = ['name', 'columns']
            for field in required_fields:
                if field in template:
                    quality_score += 15

            # Check column completeness (40 points)
            if 'columns' in template:
                columns = template['columns']
                if len(columns) > 0:
                    quality_score += 10

                # Check column definitions
                for col in columns:
                    if 'name' in col and 'data_type' in col:
                        quality_score += min(2, 30 // len(columns))

            # Check metadata (30 points)
            metadata_fields = ['description', 'source_dataset', 'created_at']
            for field in metadata_fields:
                if field in template:
                    quality_score += 10

            quality_percentage = min(100, quality_score)

            return {
                'template_name': template.get('name', 'Unknown'),
                'quality_score': quality_percentage,
                'column_count': len(template.get('columns', [])),
                'has_metadata': 'description' in template,
                'has_relationships': 'relationships' in template,
                'status': 'Excellent' if quality_percentage >= 80 else 'Good' if quality_percentage >= 60 else 'Fair'
            }

        except Exception as e:
            self.logger.error(f"Template analysis failed: {e}")
            return {'error': str(e), 'quality_score': 0}
